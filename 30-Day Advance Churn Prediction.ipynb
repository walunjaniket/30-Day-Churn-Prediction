{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1165d09d-0b72-4708-8619-78c5d853ffed",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5149e4-096e-406d-aa13-28230b8dd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "    StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4dc1c-aebf-4000-a82f-db467d2b1a7c",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604518af-e7f4-4e11-b620-30d6594d68d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original DataFrame: \n",
      "   Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "3  489434     22041         RECORD FRAME 7\" SINGLE SIZE         48   \n",
      "4  489434     21232       STRAWBERRY CERAMIC TRINKET BOX        24   \n",
      "\n",
      "           InvoiceDate  Price  Customer ID         Country  \n",
      "0  2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1  2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2  2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "3  2009-12-01 07:45:00   2.10      13085.0  United Kingdom  \n",
      "4  2009-12-01 07:45:00   1.25      13085.0  United Kingdom  \n",
      "\n",
      "Inspect the total number of rows in the DataFrame:  1067371\n",
      "\n",
      " Invoice         object\n",
      "StockCode       object\n",
      "Description     object\n",
      "Quantity         int64\n",
      "InvoiceDate     object\n",
      "Price          float64\n",
      "Customer ID    float64\n",
      "Country         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and inspect number of rows\n",
    "df = pd.read_csv(\"online_retail_II.csv\")\n",
    "\n",
    "print(\"\\nOriginal DataFrame: \\n\", df.head(5))\n",
    "print(\"\\nInspect the total number of rows in the DataFrame: \", len(df))\n",
    "\n",
    "# Check the datatypes\n",
    "print(\"\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5caa0-35ce-4f4d-95f1-1e2eadd88992",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6bff28-be69-431a-a29a-bef3accfe440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Invoice                object\n",
      "StockCode              object\n",
      "Description            object\n",
      "Quantity                int64\n",
      "InvoiceDate    datetime64[ns]\n",
      "Price                 float64\n",
      "Customer ID             int64\n",
      "Country                object\n",
      "totalamount           float64\n",
      "dtype: object\n",
      "\n",
      "Cleaned DataFrame has been successfully saved to a csv file.\n",
      "\n",
      "Total number of rows after data cleaning: 805531\n"
     ]
    }
   ],
   "source": [
    "## Data cleaning\n",
    "# Drop columns where Customer ID is missing\n",
    "df = df.dropna(subset=[\"Customer ID\"])\n",
    "\n",
    "# Remove rows where Quantity <= 0 or Price <= 0\n",
    "mask = (df[\"Quantity\"] <= 0) | (df[\"Price\"] < 0.01)\n",
    "df = df[~mask]\n",
    "\n",
    "# Convert Customer ID to integer\n",
    "df[\"Customer ID\"] = df[\"Customer ID\"].astype(int)\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n",
    "\n",
    "## Create new features\n",
    "# Create total_amount feature\n",
    "df[\"totalamount\"] = (df[\"Quantity\"] * df[\"Price\"]).round(2)\n",
    "\n",
    "# Recheck the dtypes\n",
    "print(\"\\n\", df.dtypes)\n",
    "\n",
    "# Make all the feature names lowercase for consistency.\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Make the names of all columns of consistent style\n",
    "df_cleaned = df.rename(columns={\n",
    "    \"stockcode\": \"stock_code\",\n",
    "    \"invoicedate\": \"invoice_date\",\n",
    "    \"customer id\": \"customer_id\",\n",
    "    \"totalamount\": \"total_amount\"\n",
    "    })\n",
    "\n",
    "# Save the cleaned dataset to a csv file\n",
    "df_cleaned.to_csv(\"cleaned_dataframe.csv\", index=False)\n",
    "print(\"\\nCleaned DataFrame has been successfully saved to a csv file.\")\n",
    "\n",
    "print(\"\\nTotal number of rows after data cleaning:\", df_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c4cac-106d-4466-b49e-17d08530a2f8",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8250408e-4d66-457d-9356-24c872aed12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated customer data has been successfully saved to a csv file.\n",
      "\n",
      "       customer_id  total_quantity  total_spend  total_orders  \\\n",
      "5692        18102          188340    608821.65           145   \n",
      "2277        14646          367193    528602.52           151   \n",
      "1789        14156          165992    313946.37           156   \n",
      "2538        14911          149987    295972.63           398   \n",
      "5050        17450           84720    246973.09            51   \n",
      "\n",
      "               first_buy            last_buy  \n",
      "5692 2009-12-01 09:24:00 2011-12-09 11:50:00  \n",
      "2277 2009-12-02 16:52:00 2011-12-08 12:12:00  \n",
      "1789 2009-12-01 12:30:00 2011-11-30 10:54:00  \n",
      "2538 2009-12-01 11:41:00 2011-12-08 15:54:00  \n",
      "5050 2010-09-27 16:59:00 2011-12-01 13:29:00  \n",
      "Snapshot 2011-06-01: Churn rate = 51.12% (411 churned out of 804)\n",
      "Snapshot 2011-07-01: Churn rate = 52.98% (436 churned out of 823)\n",
      "Snapshot 2011-08-01: Churn rate = 53.17% (469 churned out of 882)\n",
      "\n",
      "Master rows: 2509\n",
      "Overall churn rate: 0.5245\n",
      "\n",
      "Churn by driver:\n",
      " driver\n",
      "adoption       0.654412\n",
      "price          0.575985\n",
      "quality        0.400000\n",
      "competition    0.376840\n",
      "Name: churn, dtype: float64\n",
      "\n",
      "High-LTV customers (98th percentile):\n",
      " high_ltv\n",
      "False    2457\n",
      "True       52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Create customer summary\n",
    "# Get aggregations\n",
    "cust_summary = df_cleaned.groupby(\"customer_id\").agg(\n",
    "    total_quantity=(\"quantity\", \"sum\"),\n",
    "    total_spend=(\"total_amount\", \"sum\"),\n",
    "    total_orders=(\"invoice\", \"nunique\"),\n",
    "    first_buy=(\"invoice_date\", \"min\"),\n",
    "    last_buy=(\"invoice_date\", \"max\")\n",
    "    ).reset_index()\n",
    "\n",
    "cust_summary[\"total_spend\"] = cust_summary[\"total_spend\"].round(2)\n",
    "\n",
    "# =============================================================================\n",
    "# This can also be done like this\n",
    "# =============================================================================\n",
    "# cust_summary = df_cleaned.groupby(\"customer_id\").agg({\n",
    "#     \"quantity\": \"sum\",\n",
    "#     \"total_amount\": \"sum\",\n",
    "#     \"invoice\": \"nunique\",\n",
    "#     \"invoice_date\": [\"min\", \"max\"]\n",
    "#     }).reset_index()\n",
    "\n",
    "# cust_summary.columns = [\"customer_id\", \"total_quntity\", \"total_spend\",\n",
    "                         # \"total_orders\", \"first_buy\", \"last_buy\"]\n",
    "\n",
    "# Save the aggregated customer data to a csv file\n",
    "cust_summary.to_csv(\"customer_summary.csv\", index=False)\n",
    "print(\"Aggregated customer data has been successfully saved to a csv file.\")\n",
    "\n",
    "# Top customers by spend\n",
    "print(\"\\n\",\n",
    "      cust_summary.sort_values(by=\"total_spend\", ascending=False).head(5))\n",
    "\n",
    "## Feature engineering\n",
    "# Define a snapshot date\n",
    "snapshot_dates = pd.date_range('2011-06-01', '2011-08-01', freq='MS')\n",
    "# Jun, Jul, Aug\n",
    "all_dfs = []\n",
    "\n",
    "for snapshot in snapshot_dates:\n",
    "    ## SNAPSHOT LOOP: Build one month's churn data\n",
    "    # 1. Set time windows for this snapshot\n",
    "    start_90 = snapshot - timedelta(days=90)   # Look back 90 days\n",
    "    end_90 = snapshot                          # Snapshot \"today\"\n",
    "    start_30 = snapshot + timedelta(days=1)    # Predict next 30 days\n",
    "    end_30 = snapshot + timedelta(days=30)\n",
    "\n",
    "    # 2. Find active customers in past 90 days\n",
    "    active = df_cleaned[(df_cleaned[\"invoice_date\"] >= start_90) &\n",
    "                        (df_cleaned[\"invoice_date\"] <= end_90)]\n",
    "\n",
    "    # 3. Filter loyal customers: >=3 purchases in 90 days\n",
    "    purchases_90 = active.groupby(\"customer_id\")[\"invoice\"].nunique()\n",
    "    loyal_ids = purchases_90[purchases_90 >= 2].index\n",
    "    active_customers = active[\"customer_id\"].unique()\n",
    "    active_customers = [cid for cid in active_customers if cid in loyal_ids]\n",
    "\n",
    "    # 4. Check who comes back in next 30 days\n",
    "    future = df_cleaned[(df_cleaned[\"invoice_date\"] >= start_30) &\n",
    "                        (df_cleaned[\"invoice_date\"] <= end_30)]\n",
    "    still_active = future[\"customer_id\"].unique()\n",
    "\n",
    "    # 5. Label churn: 1 = gone, 0 = stayed\n",
    "    labels = [{\"customer_id\": cid, \"churn\": 0 if cid in still_active else 1}\n",
    "              for cid in active_customers]\n",
    "    label_df = pd.DataFrame(labels)\n",
    "    churn_rate = label_df['churn'].mean()\n",
    "    print(f\"Snapshot {snapshot.date()}: Churn rate = {churn_rate:.2%}\\\n",
    " ({label_df['churn'].sum()} churned out of {len(label_df)})\")\n",
    "\n",
    "    # 6. Keep only loyal customers' full transaction data\n",
    "    observed_data = active[active[\"customer_id\"].isin(loyal_ids)].copy()\n",
    "\n",
    "    # 7. Build features: spend, orders, basket size\n",
    "    cust_obs = observed_data.groupby(\"customer_id\").agg({\n",
    "        \"total_amount\": \"sum\",\n",
    "        \"invoice\": \"nunique\",\n",
    "        \"quantity\": \"sum\"\n",
    "    }).reset_index()\n",
    "    cust_obs[\"avg_basket\"] = cust_obs[\"total_amount\"] / cust_obs[\"invoice\"]\n",
    "\n",
    "    # 8. Split 90 days: early vs late spend\n",
    "    mid_date = start_90 + timedelta(days=45)\n",
    "    early = observed_data[observed_data[\"invoice_date\"] < mid_date]\n",
    "    late = observed_data[observed_data[\"invoice_date\"] >= mid_date]\n",
    "    early_spend = early.groupby(\"customer_id\")[\"total_amount\"]\\\n",
    "        .sum().reset_index(name=\"spend_early\")\n",
    "    late_spend = late.groupby(\"customer_id\")[\"total_amount\"]\\\n",
    "        .sum().reset_index(name=\"spend_late\")\n",
    "    cust_obs = cust_obs.merge(early_spend, on=\"customer_id\",\n",
    "                              how=\"left\").fillna(0)\n",
    "    cust_obs = cust_obs.merge(late_spend, on=\"customer_id\",\n",
    "                              how=\"left\").fillna(0)\n",
    "    cust_obs[\"spend_drop\"] = (cust_obs[\"spend_early\"] /\\\n",
    "                              (cust_obs[\"spend_late\"] + 1)).round(2)\n",
    "\n",
    "    # 9. Add country + lifetime spend\n",
    "    country = observed_data.groupby(\"customer_id\")[\"country\"]\\\n",
    "        .first().reset_index()\n",
    "    cust_obs = cust_obs.merge(country, on=\"customer_id\")\n",
    "    total_spend = df_cleaned.groupby(\"customer_id\")[\"total_amount\"]\\\n",
    "        .sum().reset_index(name=\"total_spend\")\n",
    "    cust_obs = cust_obs.merge(total_spend, on=\"customer_id\")\n",
    "\n",
    "    # 10. Assign churn driver (why they left)\n",
    "    driver = []\n",
    "    for _, row in cust_obs.iterrows():\n",
    "        if row[\"avg_basket\"] < 25:\n",
    "            d = \"quality\"      # Low value per order\n",
    "        elif row[\"spend_drop\"] > 2.5:\n",
    "            d = \"price\"        # Big spend drop\n",
    "        elif row[\"invoice\"] < 3:\n",
    "            d = \"adoption\"     # Never repeated\n",
    "        else:\n",
    "            d = \"competition\"  # Switched elsewhere\n",
    "        driver.append(d)\n",
    "    cust_obs[\"driver\"] = driver\n",
    "\n",
    "    # 11. Build final row for this snapshot\n",
    "    final_df = cust_obs[['customer_id', 'avg_basket', 'invoice',\n",
    "                         'total_amount', 'spend_drop', 'total_spend',\n",
    "                         'country', 'driver']]\n",
    "    final_df = final_df.merge(label_df, on=\"customer_id\")\n",
    "    threshold = final_df[\"total_spend\"].quantile(0.98)\n",
    "    final_df[\"high_ltv\"] = final_df[\"total_spend\"] > threshold\n",
    "    final_df = final_df.assign(snapshot=snapshot)  # Tag with month\n",
    "\n",
    "    # 12. Save this month's data\n",
    "    all_dfs.append(final_df)\n",
    "\n",
    "# Concat all snapshots into master dataset\n",
    "master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "master_df.to_csv(\"master_dataset.csv\", index=False)\n",
    "\n",
    "# Final summary (once, on full data)\n",
    "print(\"\\nMaster rows:\", len(master_df))\n",
    "print(\"Overall churn rate:\", master_df['churn'].mean().round(4))\n",
    "print(\"\\nChurn by driver:\\n\",\n",
    "      master_df.groupby('driver')['churn'].mean().sort_values(ascending=False))\n",
    "print(\"\\nHigh-LTV customers (98th percentile):\\n\",\n",
    "      master_df['high_ltv'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cfda6-5054-4c49-8a92-822c368930e0",
   "metadata": {},
   "source": [
    "# Build Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986e06a7-5066-446b-9ccb-8bdbcf7256be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision of base model @ top 10%:\n",
      "Precision: 0.529 | AUC: 0.722\n"
     ]
    }
   ],
   "source": [
    "## Build model\n",
    "# Feature selection\n",
    "X = master_df.drop([\"customer_id\", \"churn\", \"country\",\n",
    "                    \"snapshot\", \"high_ltv\"], axis=1)\n",
    "y = master_df[\"churn\"]\n",
    "\n",
    "# Encode \"driver\" column\n",
    "X = pd.get_dummies(X, columns=[\"driver\"], prefix=\"driver\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=25,\n",
    "    stratify=y\n",
    "    )\n",
    "\n",
    "# Instantiate model\n",
    "model = xgb.XGBClassifier(random_state=25,\n",
    "                          scale_pos_weight=(y==0).sum()/(y==1).sum(),\n",
    "                          # use_label_encoder=False,  # Depricated\n",
    "                          eval_metric=\"logloss\",\n",
    "                          n_jobs=-1\n",
    "                          )\n",
    "# Balance churn with more weight to y = 1(churned)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "base_proba = model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(base_proba, 0.90)  # Top 10% @ risk\n",
    "base_y_pred = (base_proba >= threshold).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, base_y_pred)  # Compare the true positives\n",
    "auc = roc_auc_score(y_test, base_proba)  # Compare probabilities\n",
    "print(\"\\nPrecision of base model @ top 10%:\")\n",
    "print(f\"Precision: {precision:.3f} | AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819db0d-f203-49b2-9ff8-deff8a11a717",
   "metadata": {},
   "source": [
    "### AUC Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43955ef6-60e4-4374-a550-dc50a6402430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC after hyperparameter tuning:\n",
      "Precision @ top 10%: 0.525 | AUC(Tuned): 0.758\n"
     ]
    }
   ],
   "source": [
    "# Tuning for AUC\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 150, 200, 250],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.09, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"min_child_weight\": [1, 2, 3]\n",
    "    }\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=25)\n",
    "\n",
    "grid_auc = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # Optimize roc across folds\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    "    )\n",
    "\n",
    "#Fit the grid on training dataset\n",
    "# grid_auc.fit(X_train, y_train)\n",
    "\"\"\"Run grid search only when needed. Avoid unnecessary computing expense.\"\"\"\n",
    "\n",
    "# print(\"\\nBest parameters: \", grid_auc.best_params_)\n",
    "# print(\"\\nBest CV AUC: \", grid_auc.best_score_)\n",
    "\"\"\"Only print along with the grid search. Else it thorws error.\"\"\"\n",
    "\n",
    "auc_model = xgb.XGBClassifier(random_state=25,\n",
    "                          scale_pos_weight=(y==0).sum()/(y==1).sum(),\n",
    "                          # use_label_encoder=False,  # Depricated\n",
    "                          colsample_bytree=1.0,\n",
    "                          learning_rate=0.01,\n",
    "                          max_depth=4,\n",
    "                          min_child_weight=1,\n",
    "                          n_estimators=200,\n",
    "                          subsample=0.7,\n",
    "                          eval_metric=\"logloss\",\n",
    "                          n_jobs=-1\n",
    "                          )\n",
    "\n",
    "# Fit model\n",
    "auc_model.fit(X_train, y_train)\n",
    "\n",
    "proba = auc_model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(proba, 0.90)  # Top 10% @ risk\n",
    "y_pred = (proba >= threshold).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)  # Compare the true positives\n",
    "auc = roc_auc_score(y_test, proba)  # Compare probabilities\n",
    "print(\"\\nAUC after hyperparameter tuning:\")\n",
    "print(f\"Precision @ top 10%: {precision:.3f} | AUC(Tuned): {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa788822-537e-449a-8fd8-4bddd842a814",
   "metadata": {},
   "source": [
    "### Precision Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c66f6089-4e89-4fc5-997c-3199faaa0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final tuned model performance after precision hyperparameter tuning:\n",
      "Precision @ top 10%: 0.882 | AUC: 0.759\n"
     ]
    }
   ],
   "source": [
    "# Define a scorer for Precision @ top 10%\n",
    "\n",
    "\n",
    "def precision_at_top_10(y_true, y_pred_proba):\n",
    "    \"\"\"Compute Precision @ top 10% using predicted probabilities.\"\"\"\n",
    "    if len(y_pred_proba) == 0 or np.all(np.isnan(y_pred_proba)):\n",
    "        return 0.0\n",
    "    threshold = np.percentile(y_pred_proba, 90)\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    if y_pred.sum() == 0:\n",
    "        return 0.0\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "# Create scorer: needs_proba=False, pass proba via predict_proba\n",
    "precision_scorer = make_scorer(\n",
    "    precision_at_top_10,\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid_precision = {\n",
    "    \"n_estimators\": [150, 200, 250],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1.0],\n",
    "    \"min_child_weight\": [1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=25)\n",
    "\n",
    "# Grid search\n",
    "grid_precision = GridSearchCV(\n",
    "    estimator=auc_model,\n",
    "    param_grid=param_grid_precision,\n",
    "    scoring=precision_scorer,\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "# grid_precision.fit(X_train, y_train)\n",
    "\"\"\"Run grid search only when needed. Avoid unnecessary computing expense.\"\"\"\n",
    "\n",
    "# print(\"\\nBest parameters (Precision tuning): \", grid_precision.best_params_)\n",
    "# print(\"Best CV Precision @ top 10%: \", grid_precision.best_score_)\n",
    "\"\"\"Only print along with the grid search. Else it thorws error.\"\"\"\n",
    "\n",
    "# Final model\n",
    "final_model = xgb.XGBClassifier(\n",
    "    random_state=25,\n",
    "    scale_pos_weight=(y == 0).sum() / (y == 1).sum(),\n",
    "    eval_metric=\"logloss\",\n",
    "    colsample_bytree=1.0,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=200,\n",
    "    subsample=0.7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "final_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(final_proba, 90)\n",
    "final_y_pred = (final_proba >= threshold).astype(int)\n",
    "final_precision = precision_score(y_test, final_y_pred, zero_division=0)\n",
    "final_auc = roc_auc_score(y_test, final_proba)\n",
    "\n",
    "print(\"\\nFinal tuned model performance after precision hyperparameter tuning:\")\n",
    "print(f\"Precision @ top 10%: {final_precision:.3f} | AUC: {final_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3392e7-af97-4b09-b457-3ac72a1578d5",
   "metadata": {},
   "source": [
    "### Time-split model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e45b748-afad-4401-926c-d8a7b8726f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-split Precision @ 10%:\n",
      "Precision: 0.843 | AUC: 0.746\n",
      "\n",
      "Final tuned model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Sort master_df by snapshot date\n",
    "master_df = master_df.sort_values('snapshot')\n",
    "\n",
    "# Use June/July as train, August as test\n",
    "train_idx = master_df['snapshot'] < '2011-08-01'\n",
    "test_idx = master_df['snapshot'] == '2011-08-01'\n",
    "\n",
    "X_train = X.loc[train_idx]\n",
    "X_test = X.loc[test_idx]\n",
    "y_train = y.loc[train_idx]\n",
    "y_test = y.loc[test_idx]\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "proba = final_model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(proba, 90)\n",
    "precision = precision_score(y_test, proba >= threshold)\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "print(\"\\nTime-split Precision @ 10%:\")\n",
    "print(f\"Precision: {precision:.3f} | AUC: {auc:.3f}\")\n",
    "\n",
    "# Save the final tuned model\n",
    "joblib.dump(final_model, \"30_day_churn_model_v1.pkl\")\n",
    "joblib.dump(X_train.columns.tolist(), \"model_features_v1.pkl\")\n",
    "print(\"\\nFinal tuned model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec073949-ffab-4d8f-8c3b-1a41f80c12d5",
   "metadata": {},
   "source": [
    "# Business Impact Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd3cae-6739-44ee-9ec5-0e706494df97",
   "metadata": {},
   "source": [
    "# 30-Day Advance Churn Prediction - Business Impact Summary\n",
    "\n",
    "## Project Goal\n",
    "Predict which customers will **not make a purchase in the next 30 days** — **30 days in advance** — to enable targeted retention campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Model Performance (Time-Split Validation: Aug 2011)\n",
    "\n",
    "| Metric | Value | Business Meaning |\n",
    "|--------|-------|------------------|\n",
    "| **Precision @ Top 10% Risk** | **0.843** | **84.3% of customers flagged as high-risk actually churn** |\n",
    "| **AUC** | **0.746** | Strong overall ranking power |\n",
    "| **Validation** | Time-split (June/July → August) | No data leakage. Real future performance. |\n",
    "\n",
    "> **Only 1 in 6.3 interventions is a false positive.**\n",
    "\n",
    "---\n",
    "\n",
    "## Expected ROI (Per 1,000 Customers Targeted)\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| Customers contacted | 1,000 |\n",
    "| True churners caught | **843** |\n",
    "| False positives | 157 |\n",
    "| Avg. CLV saved per prevented churn | \\$200 |\n",
    "| Cost per intervention (email/call/discount) | \\$5 |\n",
    "| **Total cost** | \\$5,000 |\n",
    "| **Revenue saved** | \\$168,600 |\n",
    "| **Net profit** | **\\$163,600** |\n",
    "\n",
    "> **ROI: 32.7** — **32.70 dollars profit per 1 dollar spent**\n",
    "\n",
    "---\n",
    "\n",
    "## Model Artifacts (Saved)\n",
    "- `30_day_churn_model_v1.pkl` → Trained XGBoost model  \n",
    "- `model_features_v1.pkl` → Required feature list\n",
    "\n",
    "---\n",
    "\n",
    "**End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988e2b4-aa30-46d4-b6dc-f26f598a12c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
